{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_shakespeare.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpjHf2WSWKUhuHLVoQoJa4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "!pip install torchtyping\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install GPUtil\n",
        "!pip install jsonlines"
      ],
      "metadata": {
        "id": "dPSJPPzGpa7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "id": "jNlGvRav2QBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F86TfZBkpHF2"
      },
      "outputs": [],
      "source": [
        "from gpt2 import GPT2\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import random\n",
        "from torch.nn import functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "UHoWyybqzDYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "        \n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "tTvezJd5p0kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128"
      ],
      "metadata": {
        "id": "BS7m9jXlqGn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
        "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
        "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCsIKDJoq1-e",
        "outputId": "34c4bd6d-9cbb-44f7-e95b-3b120caee180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1115394 characters, 65 unique.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, shuffle=True, pin_memory=True, batch_size=batch_size\n",
        ")\n",
        "print(\"train loader:\", train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyKZdjyYrgGn",
        "outputId": "e00ca672-c84d-4bda-a4b5-5e2e5e624705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loader: <torch.utils.data.dataloader.DataLoader object at 0x7fad248ffd10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "\n",
        "model = GPT2(\n",
        "    num_layers=8,\n",
        "    num_heads=8,\n",
        "    vocab_size=train_dataset.vocab_size,\n",
        "    hidden_size=512, #corresponds to n_embd\n",
        "    max_position_embeddings=train_dataset.block_size, # corresponds to block_size\n",
        "    dropout=0.1,\n",
        "    layer_norm_epsilon=1e-5,\n",
        ").to(device).train()\n",
        "\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 6e-4\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldio77BxrZKB",
        "outputId": "78c99efc-6e41-4e42-f2cb-37b4df429290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "number of parameters: 25318912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 2\n",
        "\n",
        "#counter used for lr decay\n",
        "tokens = 0\n",
        "warmup_tokens = 512 * 20\n",
        "final_tokens = 2*len(train_dataset)*block_size\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(max_epochs):\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for it, (x, y) in pbar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        gpt_output = model(x)\n",
        "        loss = loss_fn(gpt_output.logits.view(-1, gpt_output.logits.size(-1)), y.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
        "        if tokens < warmup_tokens:\n",
        "            # linear warmup\n",
        "            lr_mult = float(tokens) / float(max(1, warmup_tokens))\n",
        "        else:\n",
        "            # cosine learning rate decay\n",
        "            progress = float(tokens - warmup_tokens) / float(max(1, final_tokens - warmup_tokens))\n",
        "            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "        lr = learning_rate * lr_mult\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "            \n",
        "        pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"\\n training time:\", end_time - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIT3j07zsDJo",
        "outputId": "bdd11676-e12a-4cf1-8e71-a1c1680c9eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 iter 8713: train loss 0.71790: 100%|██████████| 8714/8714 [2:06:38<00:00,  1.15it/s]\n",
            "epoch 2 iter 8713: train loss 0.31412: 100%|██████████| 8714/8714 [2:07:11<00:00,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " training time: 15230.299560308456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_logits(logits, k):\n",
        "    v, ix = torch.topk(logits, k)\n",
        "    out = logits.clone()\n",
        "    out[out < v[:, [-1]]] = -float('Inf')\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
        "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
        "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
        "    of block_size, unlike an RNN that has an infinite context window.\n",
        "    \"\"\"\n",
        "    block_size = model.get_block_size()\n",
        "    model.eval()\n",
        "    for k in range(steps):\n",
        "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
        "        logits = model(x_cond).logits\n",
        "        # pluck the logits at the final step and scale by temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop probabilities to only the top k options\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # sample from the distribution or take the most likely\n",
        "        if sample:\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        # append to the sequence and continue\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "xnMVXppQ28oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"O God, O God!\"\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(device)\n",
        "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "print(completion)"
      ],
      "metadata": {
        "id": "5NGczd5KwwYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfc13b0d-49ae-4337-c582-d9b455e0fb16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O God, O God! that every fair!\n",
            "\n",
            "HORTENSIO:\n",
            "Farewell, and shall go wash the canopy.\n",
            "\n",
            "KATHARINA:\n",
            "Alas, I warrant you: I will not do't, and work.\n",
            "\n",
            "PETRUCHIO:\n",
            "You were more at least twinning some supping things\n",
            "My weak needly finger stars in a dreams,\n",
            "On all best all the flatterments of me,\n",
            "That I, being govern'd by the watery moon,\n",
            "May send forth plenteous tears to drown the world!\n",
            "Oh for my husband, for my dear lord Edward!\n",
            "\n",
            "Children:\n",
            "Oh for our father, for our dear lord Clarence!\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Alas for both, both mine, Edward and Clarence!\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "What stay had I but Edward? and he's gone.\n",
            "\n",
            "Children:\n",
            "What stay had we but Clarence? and he's gone.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "What stays had I but they? and they are gone.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Was never will before with thee?\n",
            "\n",
            "BAPTISTA:\n",
            "No, my good lord, Lord Northumberland.\n",
            "\n",
            "KING RICHARD III:\n",
            "Then call them to our praise them.\n",
            "\n",
            "BRUTUS:\n",
            "I never saw you hear have.\n",
            "\n",
            "SICINIUS:\n",
            "We know you well?\n",
            "\n",
            "MENENIUS:\n",
            "Yet you are general, by this good wills,--\n",
            "\n",
            "SICINIUS:\n",
            "'Tis prince then learns that thou dost know it,\n",
            "Who hath not so much to the maid of mine.\n",
            "\n",
            "MENENIUS:\n",
            "He's a lamb. Sadise\n",
            "To the fearful bendying his foe support\n",
            "\n",
            "MARCIUS:\n",
            "I know him all our wits well; every friends,\n",
            "Bristness to win the heavens, the your cry,\n",
            "The city is an egg from officer.\n",
            "\n",
            "Boatswain:\n",
            "When they are they are the lusty compassion here\n",
            "I heard them seats.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "Let not honourable shut him such a corse,\n",
            "Persuade him that he hath been troubled with born.\n",
            "\n",
            "TRANIO:\n",
            "Thou, trust to the Henry.\n",
            "\n",
            "PROSPERO:\n",
            "Poor soul, thy father, thy father, thy face is a proper,\n",
            "I thought the single state where no man envious guilty\n",
            "To plant upon thyself and us to a house.\n",
            "\n",
            "Third Servant:\n",
            "We shall have no hear thee took the love.\n",
            "\n",
            "CAPULET:\n",
            "So then do I turn to thy business, my lord;\n",
            "Which else you do me with a grain rage\n",
            "To keep upon my head; and I employ'd,\n",
            "If they did be solicit me and love their childer\n",
            "As friends will fetch the your friends.\n",
            "\n",
            "MERCUTIO:\n",
            "If love be rough with you\n"
          ]
        }
      ]
    }
  ]
}